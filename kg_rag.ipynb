{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/princee1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/princee1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/princee1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/princee1/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/princee1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/princee1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/princee1/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional: For better coverage of languages\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('?')\n",
    "stop_words.add('What')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_data  = pd.read_csv('data/texts.csv')\n",
    "corpus_text = pd.read_csv('corpus_text.csv')\n",
    "open_ie = pd.read_csv('rel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal,TypedDict\n",
    "import re\n",
    "\n",
    "\n",
    "possible_tags = [\n",
    "        'ARG0','ARG1', 'ARG2', 'ARG3','ARG4','ARG5', 'ARGM-TMP', 'ARGM-PRD', 'ARGM-MNR', \n",
    "        'ARGM-LOC', 'ARGM-DIR', 'ARGM-NEG', 'ARGM-ADV', 'ARGM-MOD', \n",
    "        'ARGM-CAU', 'V','ARGM-DIS'\n",
    "    ]\n",
    "ArgIdentifiers = Literal['ARG0','ARG1', 'ARG2', 'ARG3','ARG4','ARG5','ARGM-TMP','ARGM-PRD','ARGM-MNR'\n",
    "                         ,'ARGM-LOC','ARGM-DIR','ARGM-NEG','ARGM-ADV','ARGM-MOD','ARGM-CAU']\n",
    "args_noun = ['ARG0','ARG1', 'ARG2', 'ARG3','ARG4','ARG5']\n",
    "args_info = ['ARGM-TMP','ARGM-PRD','ARGM-MNR','ARGM-LOC']\n",
    "args_neg = ['ARGM-NEG']\n",
    "args_v_plus =['ARGM-DIR','ARGM-NEG','ARGM-ADV','ARGM-MOD']\n",
    "Neo4JNodeType = Literal['ARGM-TMP','ARGM-PRD','ARGM-MNR','ARGM-LOC','ARGS']\n",
    "OpenIE_REGEX = r'\\[(ARG\\d*|ARGM-[A-Z]+|V):\\s*(.+?)\\]'\n",
    "\n",
    "Subj_Label = 'F-ARG'\n",
    "\n",
    "class NoVerbException(Exception):\n",
    "    ...\n",
    "\n",
    "\n",
    "class NoArgsException(Exception):\n",
    "    ...\n",
    "\n",
    "class NoNeighborhoodException(Exception):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "from allennlp_models.structured_prediction.predictors.openie import OpenIePredictor\n",
    "from typing import Dict\n",
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceInformation:\n",
    "    def __init__(self,sentence):\n",
    "        self.sentence = sentence\n",
    "\n",
    "        self.info:Dict[ArgIdentifiers,list[str]] = self.extract_openie_information()\n",
    "        \n",
    "        self._build_verb()\n",
    "        self._order_args()\n",
    "        \n",
    "    \n",
    "    def _build_verb(self):\n",
    "        if not self.info['V']:\n",
    "            raise NoVerbException\n",
    "        self.verb = self.info['V'][0].strip()\n",
    "        self.verb_plus:str = ' '.join(self.info['ARGM-MOD'])+' '.join(self.info['ARGM-ADV'])+' '.join(self.info['ARGM-NEG'])+ self.verb+' '.join(self.info['ARGM-DIR'])\n",
    "        self.verb_plus = self.verb_plus.strip()\n",
    "        \n",
    "         \n",
    "    def extract_openie_information(self):\n",
    "\n",
    "        openie_info = {tag: [] for tag in possible_tags}\n",
    "        \n",
    "        matches = re.findall(OpenIE_REGEX, self.sentence)\n",
    "        \n",
    "        for match in matches:\n",
    "            if len(match) >= 2:\n",
    "                tag = match[0]\n",
    "                value = match[1]\n",
    "                openie_info[tag].append(value.strip())\n",
    "        \n",
    "        for tag in openie_info:\n",
    "            if tag in args_noun:\n",
    "                if not openie_info[tag]:\n",
    "                    openie_info[tag] = None\n",
    "                else:\n",
    "                    temp = openie_info[tag]\n",
    "                    temp = ' '.join(temp)\n",
    "                    openie_info[tag] =temp\n",
    "\n",
    "        return openie_info\n",
    "    \n",
    "    @property\n",
    "    def same_v_plus(self):\n",
    "        return self.verb_plus == self.verb\n",
    "\n",
    "\n",
    "    def _order_args(self):\n",
    "        temp =[]\n",
    "        for tag in args_noun:\n",
    "            if self.info[tag] != None:\n",
    "                temp.append(self.info[tag])\n",
    "\n",
    "        self.first_arg = temp[0]\n",
    "        self.other_args = temp[1:]\n",
    "        if self.first_arg == '':\n",
    "            raise NoArgsException\n",
    "    @property\n",
    "    def neighborhood(self):\n",
    "        temp =[ ]\n",
    "\n",
    "        for tag in args_info:\n",
    "            temp.extend([(tag,t)for t  in self.info[tag].copy()])\n",
    "            \n",
    "        temp.extend([('ARGS',t) for t in  self.other_args])\n",
    "        if not temp:\n",
    "            raise NoNeighborhoodException\n",
    "        return temp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alpha_numeric(word):\n",
    "    pattern = r'(?=.*[a-zA-Z])(?=.*\\d)'\n",
    "    if re.match(pattern, word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "T = []\n",
    "\n",
    "for text in texts_data['text']:\n",
    "    T.extend(word_tokenize(text))\n",
    "\n",
    "token_freq = FreqDist(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreference_predictor =  Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "openie_predictor = OpenIePredictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class KGOpenIExtractor:\n",
    "\n",
    "    def __init__(self,dataset: pd.Series,skip_dep=False,skip_coref_file=None,skip_openie_file=None,coreference_predictor:Predictor=coreference_predictor,openie_predictor:OpenIePredictor=openie_predictor,):\n",
    "        self.coreference_predictor = coreference_predictor\n",
    "        self.openie_predictor = openie_predictor\n",
    "        self.dataset = dataset\n",
    "        self.relationship:list[str] = []\n",
    "        self.corpus = []\n",
    "        self.skip_coref_file = skip_coref_file\n",
    "        self.skip_openie_file = skip_openie_file\n",
    "        self.skip_dep =skip_dep\n",
    "        \n",
    "        password = \"\"\n",
    "        neo_uri = ''\n",
    "        self.graph = Graph(neo_uri, auth=(\"neo4j\", password))\n",
    "        self.index = 0\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        if self.skip_coref_file==None:\n",
    "            self._resolve_coreference()\n",
    "            self.dataset = pd.Series(self.corpus,)\n",
    "            self.dataset.to_csv('corpus_text.csv')\n",
    "        else:\n",
    "            temp = pd.read_csv(self.skip_coref_file)\n",
    "            temp.columns = ['index','sentences']\n",
    "            self.dataset = temp['sentences']\n",
    "        print('coreference done !')\n",
    "        if self.skip_openie_file==None:\n",
    "            self._extract_information()\n",
    "        else:\n",
    "            temp = pd.read_csv(self.skip_openie_file)\n",
    "            temp.columns = ['index','sentences']\n",
    "            self.relationship = temp['sentences'].to_list()\n",
    "        \n",
    "        print('extract information done !')\n",
    "\n",
    "\n",
    "    def _co_references_resolver(self,text):\n",
    "        self.index +=1\n",
    "        print(f'{self.index}/{len(self.dataset)}')\n",
    "        try:\n",
    "            prediction = self.coreference_predictor.predict(document=text)\n",
    "            \n",
    "            words = prediction['document']\n",
    "            clusters = prediction['clusters']\n",
    "            \n",
    "            for cluster in clusters:\n",
    "                main_mention = \" \".join(words[cluster[0][0]: cluster[0][1] + 1])\n",
    "                for mention in cluster[1:]:\n",
    "                    start, end = mention\n",
    "                    words[start:end + 1] = [main_mention] + [''] * (end - start)\n",
    "            \n",
    "            resolved_text = \" \".join([word for word in words if word])\n",
    "        except:\n",
    "            self.corpus.extend(sent_tokenize(text))\n",
    "            return\n",
    "        self.corpus.extend(sent_tokenize(resolved_text))\n",
    "        return \n",
    "           \n",
    "    def _resolve_coreference(self,):\n",
    "        self.dataset.apply(self._co_references_resolver)\n",
    "    \n",
    "    def _compute_information_extraction(self,sentence):\n",
    "        output = self.openie_predictor.predict(sentence=sentence)\n",
    "        for relation in output[\"verbs\"]:\n",
    "            self.relationship.append(relation['description'])\n",
    "        self.index+=1\n",
    "        print(f'{self.index}/{len(self.dataset)}')\n",
    "        \n",
    "\n",
    "    def _extract_information(self,):\n",
    "        self.index = 0\n",
    "        if not len(self.relationship) ==0:\n",
    "            return\n",
    "        self.dataset.apply(self._compute_information_extraction)\n",
    "        pd.Series(self.relationship).to_csv('rel.csv')\n",
    "\n",
    "\n",
    "    def _split(info:str):\n",
    "        ...\n",
    "\n",
    "    def build_info_dependency(self,):\n",
    "        if self.skip_dep:\n",
    "            return\n",
    "        for rel in tqdm(self.relationship):\n",
    "            try:\n",
    "                openie_info_temp = SentenceInformation(rel) \n",
    "                \n",
    "                for triplets in openie_info_temp.neighborhood:\n",
    "\n",
    "                    obj_label,obj =triplets\n",
    "                    subj_node = Node(Subj_Label, name=openie_info_temp.first_arg)\n",
    "                    obj_node = Node(obj_label, name=obj)\n",
    "\n",
    "                    # Merge nodes into the graph\n",
    "                    self.graph.merge(subj_node, Subj_Label, \"name\")  # Merge using label and key\n",
    "                    self.graph.merge(obj_node, obj_label, \"name\")\n",
    "\n",
    "                    # Create relationship and merge into the graph\n",
    "                    \n",
    "                    self.graph.merge(Relationship(subj_node, openie_info_temp.verb, obj_node))\n",
    "                    if not openie_info_temp.same_v_plus:\n",
    "                        self.graph.merge(Relationship(subj_node, openie_info_temp.verb_plus, obj_node))\n",
    "\n",
    "\n",
    "            except Exception as  e:\n",
    "                continue\n",
    "        print('Knowledge Graph computed !')\n",
    "    \n",
    "\n",
    "    def query_node_kg(self,query,type_:Neo4JNodeType|None=None):\n",
    "        type_ = '' if type_ == None else ':'+type_\n",
    "        cypher_query = f\"\"\"\n",
    "        MATCH (n)-[r]->(m{type_})\n",
    "        WHERE n.name CONTAINS '{query}'\n",
    "        RETURN n.name AS source, type(r) AS relation, m.name AS target\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.graph.run(cypher_query).data()\n",
    "\n",
    "    def query_relation_kg(self,query):\n",
    "        query = f\"\"\"\n",
    "        MATCH (n)-[r:{query}]->(m)\n",
    "        RETURN n.name AS source, type(r) AS relation, m.name AS target\n",
    "        \"\"\"\n",
    "        return self.graph.run(query).data()\n",
    "    \n",
    "    def remove_stop_word(self,question:str):\n",
    "        filtered_words = [word for word in word_tokenize(question) if word not in stop_words]\n",
    "        temp = [word for word,tag in pos_tag(filtered_words) if tag.startswith('NN')or is_alpha_numeric(word)]\n",
    "        return temp\n",
    "\n",
    "    def generate_bigrams_trigrams(self,words, center_word):\n",
    "        center_index = words.index(center_word)\n",
    "        \n",
    "        bigrams = []\n",
    "        trigrams = []\n",
    "        \n",
    "        # Create bigrams: (center_word, previous_word), (center_word, next_word)\n",
    "        if center_index > 0:\n",
    "            bigrams.append((words[center_index - 1], words[center_index]))\n",
    "        if center_index < len(words) - 1:\n",
    "            bigrams.append((words[center_index], words[center_index + 1]))\n",
    "        \n",
    "        # Create trigrams: (previous_word, center_word, next_word)\n",
    "        if center_index > 0 and center_index < len(words) - 1:\n",
    "            trigrams.append((words[center_index - 1], words[center_index], words[center_index + 1]))\n",
    "        \n",
    "        return bigrams, trigrams\n",
    "    \n",
    "    def _get_question_subj(self,words:list[str]):\n",
    "        val = np.array([token_freq[w] for w in words])\n",
    "        index = val.argmin()\n",
    "        return words[index]\n",
    "\n",
    "    def _retrieve_KG_relations(self,questions:str):\n",
    "        words = self.remove_stop_word(questions)\n",
    "        print(words)\n",
    "        q_subj=self._get_question_subj(words)\n",
    "        print(q_subj)\n",
    "        r_q_subj = None\n",
    "        if '-' in q_subj:\n",
    "            r_q_subj = ' '.join(q_subj.split('-'))\n",
    "        \n",
    "        bigram,trigram = self.generate_bigrams_trigrams(words,q_subj)\n",
    "        \n",
    "\n",
    "        if r_q_subj != None:\n",
    "            bt,tt = self.generate_bigrams_trigrams(words,r_q_subj)\n",
    "            bigram+=bt\n",
    "            trigram+=tt\n",
    "\n",
    "        print(bigram)\n",
    "        print(trigram)\n",
    "        \n",
    "        temp_tri = []\n",
    "        for tri in trigram:\n",
    "\n",
    "            temp_tri.extend(self.query_node_kg(' '.join(tri)))\n",
    "\n",
    "        if len(temp_tri) >= 6:\n",
    "            return temp_tri\n",
    "        \n",
    "        temp_bi = []\n",
    "        for bi in bigram:\n",
    "            temp_bi.extend(self.query_node_kg(' '.join(bi)))\n",
    "\n",
    "\n",
    "        if len(temp_bi) + len(temp_tri) >= 6:\n",
    "            return temp_bi+temp_tri\n",
    "\n",
    "        if r_q_subj == None:\n",
    "            return self.query_node_kg(q_subj)\n",
    "        \n",
    "        return self.query_node_kg(q_subj)+self.query_node_kg(r_q_subj)\n",
    "        \n",
    "    \n",
    "    def retrieve_question_context(self,question:str):\n",
    "        return [rel['source']+' '+rel['relation'] +' '+rel['target']for rel in self._retrieve_KG_relations(question)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "openIE = KGOpenIExtractor(texts_data['text'],True,'corpus_text.csv','rel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coreference done !\n",
      "extract information done !\n"
     ]
    }
   ],
   "source": [
    "openIE.build()\n",
    "openIE.build_info_dependency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_val= pd.read_csv('data/questions_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(index):return question_val['question'][index],question_val['answer'][index]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF8460",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
